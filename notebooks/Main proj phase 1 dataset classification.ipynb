{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52184,"status":"ok","timestamp":1760665200580,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"GS2mLPzY-WnO","outputId":"0ae48760-a000-41e3-8f02-58c9e47b6b68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2083,"status":"ok","timestamp":1760665202669,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"80xjqD-v-qyy","outputId":"5e4d0444-53fe-4761-bdac-85583724c622"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification_Data_FINAL  test  train\tvalid\n"]}],"source":["# Check the root of your drive\n","!ls /content/drive/MyDrive/Main_project/Automobile-Defect-Detection/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15718,"status":"ok","timestamp":1760665218388,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"MsoLsCptIJxN","outputId":"337b670d-96f0-4f90-d3ec-f0ac74e193c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.12/dist-packages (0.16.1)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-hub) (2.0.2)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow-hub) (5.29.5)\n","Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow-hub) (2.19.0)\n","Requirement already satisfied: tensorflow<2.20,>=2.19 in /usr/local/lib/python3.12/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.19.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (25.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.75.1)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.5.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2025.10.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.9)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n","TensorFlow Version: 2.19.0\n","Setup complete. GPU check is next.\n"]}],"source":["# Colab Cell 1: Setup and Libraries\n","# Ensure you are running this with a GPU runtime (Runtime -> Change runtime type -> GPU)\n","!pip install tensorflow-hub\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Concatenate\n","from tensorflow.keras.models import Model\n","import tensorflow_hub as hub\n","import os\n","import numpy as np\n","\n","print(f\"TensorFlow Version: {tf.__version__}\")\n","print(\"Setup complete. GPU check is next.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1251,"status":"ok","timestamp":1760665219633,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"emGHCKdNMOFl","outputId":"6d5def58-05ad-412a-ef56-e2585599a293"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","MobileNetV2 base model loaded from tf.keras.applications. Input shape: (128, 128, 3)\n","Base model layers are set to non-trainable (frozen).\n"]}],"source":["# Colab Cell 2: Define and Load MobileNetV2 Base (COMPATIBILITY FIX)\n","\n","# Define the expected input size for MobileNetV2\n","IMG_SIZE = (128, 128)\n","IMG_SHAPE = IMG_SIZE + (3,) # (128, 128, 3) for color images\n","\n","# *** COMPATIBILITY FIX: Use Keras Applications instead of TF Hub ***\n","base_model = tf.keras.applications.MobileNetV2(\n","    input_shape=IMG_SHAPE,\n","    include_top=False,        # Exclude the classifier (we add our own)\n","    weights='imagenet'        # Load pre-trained weights\n",")\n","\n","# Freeze the base model layers\n","base_model.trainable = False\n","# We wrap it in a Keras Input so it can be called later without error\n","base_output = base_model.output\n","\n","print(f\"MobileNetV2 base model loaded from tf.keras.applications. Input shape: {IMG_SHAPE}\")\n","print(\"Base model layers are set to non-trainable (frozen).\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1760665219746,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"NpRTgFWEMzj6","outputId":"8ac2c8fa-ea71-4d3d-fd4b-33359367e9a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defect Detection Model FINALLY CORRECTED for 3 classes.\n","\n","Model Summary:\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MobileNetV2_Defect_Detector_3_Classes\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"MobileNetV2_Defect_Detector_3_Classes\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ defect_detection_GAP            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ defect_detection_dense_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                         │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ defect_detection_dropout        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                       │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output_classification_3_classes │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                         │                        │               │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_image (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ defect_detection_GAP            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ defect_detection_dense_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n","│ (\u001b[38;5;33mDense\u001b[0m)                         │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ defect_detection_dropout        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mDropout\u001b[0m)                       │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output_classification_3_classes │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m1,539\u001b[0m │\n","│ (\u001b[38;5;33mDense\u001b[0m)                         │                        │               │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,915,395</span> (11.12 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,915,395\u001b[0m (11.12 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">657,411</span> (2.51 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m657,411\u001b[0m (2.51 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# Colab Cell 3-4 (Combined): Define, Compile, and Verify CNN (FINAL CORRECTION: 3 CLASSES)\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","import numpy as np\n","\n","# Assume base_model (MobileNetV2) and IMG_SHAPE=(128, 128, 3) are defined from Cell 2\n","\n","# Change the number of classes to 3: ACCEPT, CASTING FAULT, SURFACE IMPERFECTION\n","NUM_CLASSES = 3\n","\n","# 1. Define the Input\n","input_tensor = Input(shape=IMG_SHAPE, name='input_image')\n","features_map = base_model(input_tensor, training=False)\n","\n","# --- START: Custom CNN Part for Defect Detection (The Trainable Head) ---\n","\n","# CRITICAL STEP: Flatten the feature map to a vector\n","features = GlobalAveragePooling2D(name='defect_detection_GAP')(features_map)\n","\n","# 3. Dense layer (where 'x' is first defined)\n","x = Dense(512, activation='relu', name='defect_detection_dense_1')(features)\n","\n","# 4. Dropout layer (where 'x' is updated)\n","x = Dropout(0.5, name='defect_detection_dropout')(x)\n","\n","# 5. Final Classification Layer (Output) - SET TO 3 NEURONS\n","output_tensor = Dense(NUM_CLASSES, activation='softmax', name='output_classification_3_classes')(x)\n","\n","# 6. Create the final Model\n","cnn_detection_model = Model(inputs=input_tensor, outputs=output_tensor, name='MobileNetV2_Defect_Detector_3_Classes')\n","\n","# --- END: Custom CNN Part for Defect Detection ---\n","\n","# 7. Compile the model\n","cnn_detection_model.compile(\n","    optimizer='adam',\n","    # Use SparseCategoricalCrossentropy for multi-class output\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","    metrics=['accuracy']\n",")\n","\n","print(f\"Defect Detection Model FINALLY CORRECTED for {NUM_CLASSES} classes.\")\n","\n","# Verification\n","print(\"\\nModel Summary:\")\n","cnn_detection_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32873,"status":"ok","timestamp":1760678886101,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"3r-qYf9HNbK5","outputId":"347e78b7-57f4-43ca-be73-80f5f32cee1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","--- Processing train split ---\n","Restructure Complete for train. Counts: {'CASTING FAULT': 2412, 'SURFACE IMPERFECTION': 3801, 'ACCEPT': 10704}\n","\n","--- Processing valid split ---\n","Restructure Complete for valid. Counts: {'CASTING FAULT': 102, 'SURFACE IMPERFECTION': 360, 'ACCEPT': 988}\n","\n","--- Processing test split ---\n","Restructure Complete for test. Counts: {'CASTING FAULT': 97, 'SURFACE IMPERFECTION': 180, 'ACCEPT': 511}\n","\n","-------------------------------------------\n","DATA RESTRUCTURE COMPLETE. Total Images Processed: 19155\n","-------------------------------------------\n","Found 16917 files belonging to 3 classes.\n","Found 1450 files belonging to 3 classes.\n","\n","Datasets are ready for 3-class training. You can now run Colab Cell 6.\n"]}],"source":["# Colab Cell 5: DATA RESTRUCTURING (ULTIMATE FINAL FIX) - ENCODING ADDED\n","\n","import pandas as pd\n","import os\n","import shutil\n","from google.colab import drive\n","import tensorflow as tf\n","from tensorflow.keras.utils import image_dataset_from_directory\n","\n","# 1. Mount Drive\n","drive.mount('/content/drive')\n","\n","# --- CONFIGURATION (PATHS AND CLASSES) ---\n","BASE_DATA_PATH = '/content/drive/MyDrive/Main_project/Automobile-Defect-Detection/'\n","OUTPUT_PATH = os.path.join(BASE_DATA_PATH, 'Classification_Data_FINAL/')\n","FINAL_CLASSES = ['CASTING FAULT', 'SURFACE IMPERFECTION', 'ACCEPT']\n","# ----------------------------------------------------------------------------------\n","\n","os.makedirs(OUTPUT_PATH, exist_ok=True)\n","\n","# Function to search for the image file (kept robust)\n","def find_image_file(split_path, filename):\n","    \"\"\"Searches common subdirectories for the image file.\"\"\"\n","    # Check the split folder directly\n","    direct_path = os.path.join(split_path, filename)\n","    if os.path.exists(direct_path):\n","        return direct_path\n","\n","    # Check common subfolders (in case images are tucked away)\n","    for subfolder in ['images', 'JPEGImages', 'export', 'raw']:\n","        sub_path = os.path.join(split_path, subfolder, filename)\n","        if os.path.exists(sub_path):\n","            return sub_path\n","\n","    return None\n","\n","def process_split(split_name):\n","\n","    print(f\"\\n--- Processing {split_name} split ---\")\n","\n","    # Define paths\n","    output_split_path = os.path.join(OUTPUT_PATH, split_name)\n","    split_csv_path = os.path.join(BASE_DATA_PATH, split_name, '_classes.csv')\n","    split_img_path = os.path.join(BASE_DATA_PATH, split_name)\n","\n","    # Create destination folders for the 3 classes\n","    for class_name in FINAL_CLASSES:\n","        os.makedirs(os.path.join(output_split_path, class_name), exist_ok=True)\n","\n","    try:\n","        # CRITICAL FIX: Add 'encoding' parameter for robustness\n","        df = pd.read_csv(split_csv_path, encoding='latin-1')\n","    except FileNotFoundError:\n","        print(f\"ERROR: Could not load CSV file at {split_csv_path}. Skipping.\")\n","        return 0\n","    except Exception as e:\n","        # If it still fails, print the error and stop the split\n","        print(f\"CRITICAL ERROR reading CSV: {e}\")\n","        return 0\n","\n","    counts = {c: 0 for c in FINAL_CLASSES}\n","\n","    for index, row in df.iterrows():\n","\n","        final_class = 'SKIP'\n","\n","        # 1. Determine the final class based on PRIORITY\n","        # We must check column existence safely, though Roboflow output is usually consistent.\n","        try:\n","            is_casting = row['CASTING FAULT'] == 1\n","            is_surface = row['SURFACE IMPERFECTION'] == 1\n","            is_accept = row['ACCEPT'] == 1\n","        except KeyError:\n","            # This means a column name is wrong in the CSV; we skip the row\n","            continue\n","\n","        if is_casting:\n","            final_class = 'CASTING FAULT'\n","        elif is_surface:\n","            final_class = 'SURFACE IMPERFECTION'\n","        elif is_accept and not is_casting and not is_surface:\n","            final_class = 'ACCEPT'\n","\n","        if final_class == 'SKIP': continue\n","\n","        # 2. Find and Copy the Source File\n","        filename = row['filename']\n","        src_file = find_image_file(split_img_path, filename)\n","\n","        if src_file is None:\n","            continue\n","\n","       # 3. Define and Copy the file\n","        dst_dir = os.path.join(output_split_path, final_class)\n","        dst_file = os.path.join(dst_dir, filename)\n","\n","        # ⭐️ ADDED: CHECK IF FILE ALREADY EXISTS ⭐️\n","        if os.path.exists(dst_file):\n","            # If the file is already there, just count it and skip the copy\n","            counts[final_class] += 1\n","            continue\n","        # ⭐️ END ADDED SECTION ⭐️\n","\n","        try:\n","            # Only copies if the file was NOT found above\n","            shutil.copyfile(src_file, dst_file)\n","            counts[final_class] += 1\n","        except Exception as e:\n","            # Ignore individual copy errors (which are less likely now)\n","            continue\n","\n","    print(f\"Restructure Complete for {split_name}. Counts: {counts}\")\n","    return sum(counts.values())\n","\n","# --- EXECUTE THE RESTRUCTURING ---\n","total_images = 0\n","for split in ['train', 'valid', 'test']:\n","    total_images += process_split(split)\n","\n","print(\"\\n-------------------------------------------\")\n","print(f\"DATA RESTRUCTURE COMPLETE. Total Images Processed: {total_images}\")\n","print(\"-------------------------------------------\")\n","\n","# --- FINAL DATASET LOADING ---\n","# (Loading code remains the same, assuming restructuring succeeded)\n","IMG_SIZE = (128, 128)\n","BATCH_SIZE = 32\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","def normalize_img_mobile_net(image, label):\n","    return (image / 127.5) - 1.0, label\n","\n","train_ds = image_dataset_from_directory(\n","    directory=OUTPUT_PATH + 'train',\n","    labels='inferred',\n","    label_mode='int',\n","    image_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True\n",").map(normalize_img_mobile_net).cache().shuffle(1000).prefetch(AUTOTUNE)\n","\n","val_ds = image_dataset_from_directory(\n","    directory=OUTPUT_PATH + 'valid',\n","    labels='inferred',\n","    label_mode='int',\n","    image_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False\n",").map(normalize_img_mobile_net).cache().prefetch(AUTOTUNE)\n","\n","print(\"\\nDatasets are ready for 3-class training. You can now run Colab Cell 6.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":341},"id":"8SO7LcvMO-ak","outputId":"24690374-7ae3-4041-d635-07a94b5cfe56"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m3,843\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,261,827</span> (8.63 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,261,827\u001b[0m (8.63 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> (15.01 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,843\u001b[0m (15.01 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","--- Starting MobileNet Transfer Learning (Cell 6) ---\n","Epoch 1/5\n"]},{"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nInput is empty.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_10770]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1368399779.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# 5. Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Starting MobileNet Transfer Learning (Cell 6) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nInput is empty.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_10770]"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n","\n","# --- Model Configuration ---\n","NUM_CLASSES = 3 # CASTING FAULT, SURFACE IMPERFECTION, ACCEPT\n","IMAGE_SHAPE = (128, 128, 3) # Matches the IMG_SIZE from Cell 5\n","LEARNING_RATE = 0.001\n","EPOCHS = 5 # Keep epochs low for a quick demonstration\n","# ---------------------------\n","\n","# 1. Load the pre-trained MobileNetV2 model (Feature Extractor)\n","# include_top=False means we drop the final classification layer\n","base_model = MobileNetV2(\n","    input_shape=IMAGE_SHAPE,\n","    include_top=False,\n","    weights='imagenet'\n",")\n","\n","# 2. Freeze the base model layers\n","# This prevents the pre-trained weights from changing during this initial training\n","base_model.trainable = False\n","\n","# 3. Build the classification head (New Layers for Your Task)\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(), # Reduces the feature map to a vector\n","    Dropout(0.2),            # Regularization to prevent overfitting\n","    Dense(NUM_CLASSES, activation='softmax') # Output layer for 3 classes\n","])\n","\n","# 4. Compile the model\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","    loss='sparse_categorical_crossentropy', # Used when labels are integers (label_mode='int')\n","    metrics=['accuracy']\n",")\n","\n","# Optional: Print model summary to show your teacher the frozen layers\n","model.summary()\n","\n","# 5. Train the model\n","print(\"\\n--- Starting MobileNet Transfer Learning (Cell 6) ---\")\n","history = model.fit(\n","    train_ds,\n","    epochs=EPOCHS,\n","    validation_data=val_ds\n",")\n","\n","print(\"\\nMobileNet Training Complete. You can now run the Evaluation/Inference (Cell 7).\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"collapsed":true,"executionInfo":{"elapsed":37,"status":"error","timestamp":1760666836291,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"_ty1GmaBPQcN","outputId":"0268baa4-f46e-41e6-af4b-95264f551072"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating model on validation data...\n"]},{"ename":"NameError","evalue":"name 'val_images' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2249660752.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Final Evaluation (on dummy data for now)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating model on validation data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_detection_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Loss: {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'val_images' is not defined"]}],"source":["# Colab Cell 7: Evaluate and Save the Model\n","\n","# 1. Final Evaluation (on dummy data for now)\n","print(\"Evaluating model on validation data...\")\n","loss, accuracy = cnn_detection_model.evaluate(val_images, val_labels, verbose=0)\n","print(f\"Validation Loss: {loss:.4f}\")\n","print(f\"Validation Accuracy: {accuracy:.4f}\")\n","# Note: You should replace val_images/val_labels with your REAL test set later.\n","\n","# 2. Saving the Model\n","# Save the model to your local Google Colab session storage.\n","# You will need to mount Google Drive if you want to save it permanently.\n","MODEL_SAVE_PATH = 'mobilenetv2_defect_detector.keras'\n","cnn_detection_model.save(MODEL_SAVE_PATH)\n","print(f\"\\nModel saved successfully to: {MODEL_SAVE_PATH}\")\n","print(\"This model includes the frozen MobileNetV2 base and your trained CNN detection head.\")\n","\n","# 3. (Optional) Show Training History Keys\n","# This helps if you want to plot the results later\n","print(\"\\nTraining history keys available for plotting:\")\n","print(history.history.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":568},"collapsed":true,"executionInfo":{"elapsed":215,"status":"error","timestamp":1760666827764,"user":{"displayName":"NANDHA BIJU RSET","userId":"09849500797621619532"},"user_tz":-330},"id":"j38xAimJPZ-R","outputId":"9c17802a-32d6-42b9-f74f-730550aefd08"},"outputs":[{"ename":"NameError","evalue":"name 'history' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2999822633.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss Curves'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAeUAAAFlCAYAAADVgPC6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGmVJREFUeJzt3X9M3dX9x/EX0HKpsdA6xoWyq6x1/qyWCpbR2hiXO0k0uP6xyKwpjPhjKjPam80W24K1Wjq/2pBYlFh1+oeOOmONsQR1zMaoLI20JDrbmkoVZry3Za73dlSh5Z7vH8brsLT2g/x4Q5+P5P7B6fncz7kn6JPP5V5uknPOCQAAjLvk8V4AAAD4GlEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAjPUX7rrbdUWlqqWbNmKSkpSS+//PL3HrN9+3Zddtll8vl8Ovfcc/XMM88MY6kAAExunqPc29urefPmqaGh4ZTm79+/X9dee62uuuoqdXR06O6779bNN9+s1157zfNiAQCYzJJ+yAdSJCUlaevWrVqyZMkJ56xYsULbtm3TBx98kBj7zW9+o0OHDqmlpWW4pwYAYNKZMtonaGtrUzAYHDRWUlKiu++++4TH9PX1qa+vL/F1PB7XF198oR/96EdKSkoaraUCAHBKnHM6fPiwZs2apeTkkXt51qhHORwOy+/3Dxrz+/2KxWL68ssvNW3atOOOqaur09q1a0d7aQAA/CDd3d36yU9+MmL3N+pRHo7q6mqFQqHE19FoVGeffba6u7uVnp4+jisDAECKxWIKBAKaPn36iN7vqEc5OztbkUhk0FgkElF6evqQV8mS5PP55PP5jhtPT08nygAAM0b6V6qj/j7l4uJitba2Dhp74403VFxcPNqnBgBgQvEc5f/+97/q6OhQR0eHpK/f8tTR0aGuri5JXz/1XF5enph/2223qbOzU/fcc4/27Nmjxx57TC+88IKWL18+Mo8AAIBJwnOU33vvPc2fP1/z58+XJIVCIc2fP181NTWSpM8//zwRaEn66U9/qm3btumNN97QvHnz9Mgjj+jJJ59USUnJCD0EAAAmhx/0PuWxEovFlJGRoWg0yu+UAQDjbrS6xN++BgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOGFeWGhgbl5eUpLS1NRUVF2rFjx0nn19fX6/zzz9e0adMUCAS0fPlyffXVV8NaMAAAk5XnKG/ZskWhUEi1tbXauXOn5s2bp5KSEh04cGDI+c8//7xWrlyp2tpa7d69W0899ZS2bNmie++99wcvHgCAycRzlDdu3KhbbrlFlZWVuuiii9TY2KgzzjhDTz/99JDz3333XS1atEhLly5VXl6err76at1www3fe3UNAMDpxlOU+/v71d7ermAw+O0dJCcrGAyqra1tyGMWLlyo9vb2RIQ7OzvV3Nysa6655oTn6evrUywWG3QDAGCym+Jlck9PjwYGBuT3+weN+/1+7dmzZ8hjli5dqp6eHl1xxRVyzunYsWO67bbbTvr0dV1dndauXetlaQAATHij/urr7du3a/369Xrssce0c+dOvfTSS9q2bZvWrVt3wmOqq6sVjUYTt+7u7tFeJgAA487TlXJmZqZSUlIUiUQGjUciEWVnZw95zJo1a7Rs2TLdfPPNkqRLLrlEvb29uvXWW7Vq1SolJx//c4HP55PP5/OyNAAAJjxPV8qpqakqKChQa2trYiwej6u1tVXFxcVDHnPkyJHjwpuSkiJJcs55XS8AAJOWpytlSQqFQqqoqFBhYaEWLFig+vp69fb2qrKyUpJUXl6u3Nxc1dXVSZJKS0u1ceNGzZ8/X0VFRdq3b5/WrFmj0tLSRJwBAMAwolxWVqaDBw+qpqZG4XBY+fn5amlpSbz4q6ura9CV8erVq5WUlKTVq1frs88+049//GOVlpbqwQcfHLlHAQDAJJDkJsBzyLFYTBkZGYpGo0pPTx/v5QAATnOj1SX+9jUAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjBhWlBsaGpSXl6e0tDQVFRVpx44dJ51/6NAhVVVVKScnRz6fT+edd56am5uHtWAAACarKV4P2LJli0KhkBobG1VUVKT6+nqVlJRo7969ysrKOm5+f3+/fvnLXyorK0svvviicnNz9emnn2rGjBkjsX4AACaNJOec83JAUVGRLr/8cm3atEmSFI/HFQgEdOedd2rlypXHzW9sbNT//d//ac+ePZo6deqwFhmLxZSRkaFoNKr09PRh3QcAACNltLrk6enr/v5+tbe3KxgMfnsHyckKBoNqa2sb8phXXnlFxcXFqqqqkt/v19y5c7V+/XoNDAyc8Dx9fX2KxWKDbgAATHaeotzT06OBgQH5/f5B436/X+FweMhjOjs79eKLL2pgYEDNzc1as2aNHnnkET3wwAMnPE9dXZ0yMjISt0Ag4GWZAABMSKP+6ut4PK6srCw98cQTKigoUFlZmVatWqXGxsYTHlNdXa1oNJq4dXd3j/YyAQAYd55e6JWZmamUlBRFIpFB45FIRNnZ2UMek5OTo6lTpyolJSUxduGFFyocDqu/v1+pqanHHePz+eTz+bwsDQCACc/TlXJqaqoKCgrU2tqaGIvH42ptbVVxcfGQxyxatEj79u1TPB5PjH300UfKyckZMsgAAJyuPD99HQqFtHnzZj377LPavXu3br/9dvX29qqyslKSVF5erurq6sT822+/XV988YXuuusuffTRR9q2bZvWr1+vqqqqkXsUAABMAp7fp1xWVqaDBw+qpqZG4XBY+fn5amlpSbz4q6urS8nJ37Y+EAjotdde0/Lly3XppZcqNzdXd911l1asWDFyjwIAgEnA8/uUxwPvUwYAWGLifcoAAGD0EGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGDGsKDc0NCgvL09paWkqKirSjh07Tum4pqYmJSUlacmSJcM5LQAAk5rnKG/ZskWhUEi1tbXauXOn5s2bp5KSEh04cOCkx33yySf6wx/+oMWLFw97sQAATGaeo7xx40bdcsstqqys1EUXXaTGxkadccYZevrpp094zMDAgG688UatXbtWs2fP/kELBgBgsvIU5f7+frW3tysYDH57B8nJCgaDamtrO+Fx999/v7KysnTTTTed0nn6+voUi8UG3QAAmOw8Rbmnp0cDAwPy+/2Dxv1+v8Lh8JDHvP3223rqqae0efPmUz5PXV2dMjIyErdAIOBlmQAATEij+urrw4cPa9myZdq8ebMyMzNP+bjq6mpFo9HErbu7exRXCQCADVO8TM7MzFRKSooikcig8Ugkouzs7OPmf/zxx/rkk09UWlqaGIvH41+feMoU7d27V3PmzDnuOJ/PJ5/P52VpAABMeJ6ulFNTU1VQUKDW1tbEWDweV2trq4qLi4+bf8EFF+j9999XR0dH4nbdddfpqquuUkdHB09LAwDwPzxdKUtSKBRSRUWFCgsLtWDBAtXX16u3t1eVlZWSpPLycuXm5qqurk5paWmaO3fuoONnzJghSceNAwBwuvMc5bKyMh08eFA1NTUKh8PKz89XS0tL4sVfXV1dSk7mD4UBAOBVknPOjfcivk8sFlNGRoai0ajS09PHezkAgNPcaHWJS1oAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGDGsKDc0NCgvL09paWkqKirSjh07Tjh38+bNWrx4sWbOnKmZM2cqGAyedD4AAKcrz1HesmWLQqGQamtrtXPnTs2bN08lJSU6cODAkPO3b9+uG264QW+++aba2toUCAR09dVX67PPPvvBiwcAYDJJcs45LwcUFRXp8ssv16ZNmyRJ8XhcgUBAd955p1auXPm9xw8MDGjmzJnatGmTysvLT+mcsVhMGRkZikajSk9P97JcAABG3Gh1ydOVcn9/v9rb2xUMBr+9g+RkBYNBtbW1ndJ9HDlyREePHtVZZ53lbaUAAExyU7xM7unp0cDAgPx+/6Bxv9+vPXv2nNJ9rFixQrNmzRoU9u/q6+tTX19f4utYLOZlmQAATEhj+urrDRs2qKmpSVu3blVaWtoJ59XV1SkjIyNxCwQCY7hKAADGh6coZ2ZmKiUlRZFIZNB4JBJRdnb2SY99+OGHtWHDBr3++uu69NJLTzq3urpa0Wg0cevu7vayTAAAJiRPUU5NTVVBQYFaW1sTY/F4XK2trSouLj7hcQ899JDWrVunlpYWFRYWfu95fD6f0tPTB90AAJjsPP1OWZJCoZAqKipUWFioBQsWqL6+Xr29vaqsrJQklZeXKzc3V3V1dZKkP/3pT6qpqdHzzz+vvLw8hcNhSdKZZ56pM888cwQfCgAAE5vnKJeVlengwYOqqalROBxWfn6+WlpaEi/+6urqUnLytxfgjz/+uPr7+/XrX/960P3U1tbqvvvu+2GrBwBgEvH8PuXxwPuUAQCWmHifMgAAGD1EGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGDCvKDQ0NysvLU1pamoqKirRjx46Tzv/rX/+qCy64QGlpabrkkkvU3Nw8rMUCADCZeY7yli1bFAqFVFtbq507d2revHkqKSnRgQMHhpz/7rvv6oYbbtBNN92kXbt2acmSJVqyZIk++OCDH7x4AAAmkyTnnPNyQFFRkS6//HJt2rRJkhSPxxUIBHTnnXdq5cqVx80vKytTb2+vXn311cTYz3/+c+Xn56uxsfGUzhmLxZSRkaFoNKr09HQvywUAYMSNVpemeJnc39+v9vZ2VVdXJ8aSk5MVDAbV1tY25DFtbW0KhUKDxkpKSvTyyy+f8Dx9fX3q6+tLfB2NRiV9vQkAAIy3b3rk8br2e3mKck9PjwYGBuT3+weN+/1+7dmzZ8hjwuHwkPPD4fAJz1NXV6e1a9ceNx4IBLwsFwCAUfXvf/9bGRkZI3Z/nqI8VqqrqwddXR86dEjnnHOOurq6RvTBn65isZgCgYC6u7v5dcAIYU9HFvs58tjTkRWNRnX22WfrrLPOGtH79RTlzMxMpaSkKBKJDBqPRCLKzs4e8pjs7GxP8yXJ5/PJ5/MdN56RkcE30whKT09nP0cYezqy2M+Rx56OrOTkkX1nsad7S01NVUFBgVpbWxNj8Xhcra2tKi4uHvKY4uLiQfMl6Y033jjhfAAATleen74OhUKqqKhQYWGhFixYoPr6evX29qqyslKSVF5ertzcXNXV1UmS7rrrLl155ZV65JFHdO2116qpqUnvvfeennjiiZF9JAAATHCeo1xWVqaDBw+qpqZG4XBY+fn5amlpSbyYq6ura9Dl/MKFC/X8889r9erVuvfee/Wzn/1ML7/8subOnXvK5/T5fKqtrR3yKW14x36OPPZ0ZLGfI489HVmjtZ+e36cMAABGB3/7GgAAI4gyAABGEGUAAIwgygAAGGEmynwc5Mjysp+bN2/W4sWLNXPmTM2cOVPBYPB79/905PV79BtNTU1KSkrSkiVLRneBE4zX/Tx06JCqqqqUk5Mjn8+n8847j//uv8PrntbX1+v888/XtGnTFAgEtHz5cn311VdjtFrb3nrrLZWWlmrWrFlKSko66ec1fGP79u267LLL5PP5dO655+qZZ57xfmJnQFNTk0tNTXVPP/20++c//+luueUWN2PGDBeJRIac/84777iUlBT30EMPuQ8//NCtXr3aTZ061b3//vtjvHKbvO7n0qVLXUNDg9u1a5fbvXu3++1vf+syMjLcv/71rzFeuV1e9/Qb+/fvd7m5uW7x4sXuV7/61dgsdgLwup99fX2usLDQXXPNNe7tt992+/fvd9u3b3cdHR1jvHK7vO7pc88953w+n3vuuefc/v373WuvveZycnLc8uXLx3jlNjU3N7tVq1a5l156yUlyW7duPen8zs5Od8YZZ7hQKOQ+/PBD9+ijj7qUlBTX0tLi6bwmorxgwQJXVVWV+HpgYMDNmjXL1dXVDTn/+uuvd9dee+2gsaKiIve73/1uVNc5UXjdz+86duyYmz59unv22WdHa4kTznD29NixY27hwoXuySefdBUVFUT5f3jdz8cff9zNnj3b9ff3j9USJxyve1pVVeV+8YtfDBoLhUJu0aJFo7rOiehUonzPPfe4iy++eNBYWVmZKykp8XSucX/6+puPgwwGg4mxU/k4yP+dL339cZAnmn86Gc5+fteRI0d09OjREf9D6xPVcPf0/vvvV1ZWlm666aaxWOaEMZz9fOWVV1RcXKyqqir5/X7NnTtX69ev18DAwFgt27Th7OnChQvV3t6eeIq7s7NTzc3Nuuaaa8ZkzZPNSHVp3D8laqw+DvJ0MZz9/K4VK1Zo1qxZx32Dna6Gs6dvv/22nnrqKXV0dIzBCieW4exnZ2en/v73v+vGG29Uc3Oz9u3bpzvuuENHjx5VbW3tWCzbtOHs6dKlS9XT06MrrrhCzjkdO3ZMt912m+69996xWPKkc6IuxWIxffnll5o2bdop3c+4XynDlg0bNqipqUlbt25VWlraeC9nQjp8+LCWLVumzZs3KzMzc7yXMynE43FlZWXpiSeeUEFBgcrKyrRq1So1NjaO99ImrO3bt2v9+vV67LHHtHPnTr300kvatm2b1q1bN95LO62N+5XyWH0c5OliOPv5jYcfflgbNmzQ3/72N1166aWjucwJxeuefvzxx/rkk09UWlqaGIvH45KkKVOmaO/evZozZ87oLtqw4XyP5uTkaOrUqUpJSUmMXXjhhQqHw+rv71dqauqortm64ezpmjVrtGzZMt18882SpEsuuUS9vb269dZbtWrVqhH/SMLJ7kRdSk9PP+WrZMnAlTIfBzmyhrOfkvTQQw9p3bp1amlpUWFh4VgsdcLwuqcXXHCB3n//fXV0dCRu1113na666ip1dHQoEAiM5fLNGc736KJFi7Rv377EDzeS9NFHHyknJ+e0D7I0vD09cuTIceH95ocex0cieDZiXfL2GrTR0dTU5Hw+n3vmmWfchx9+6G699VY3Y8YMFw6HnXPOLVu2zK1cuTIx/5133nFTpkxxDz/8sNu9e7erra3lLVH/w+t+btiwwaWmproXX3zRff7554nb4cOHx+shmON1T7+LV18P5nU/u7q63PTp093vf/97t3fvXvfqq6+6rKws98ADD4zXQzDH657W1ta66dOnu7/85S+us7PTvf76627OnDnu+uuvH6+HYMrhw4fdrl273K5du5wkt3HjRrdr1y736aefOuecW7lypVu2bFli/jdvifrjH//odu/e7RoaGibuW6Kcc+7RRx91Z599tktNTXULFixw//jHPxL/duWVV7qKiopB81944QV33nnnudTUVHfxxRe7bdu2jfGKbfOyn+ecc46TdNyttrZ27BdumNfv0f9FlI/ndT/fffddV1RU5Hw+n5s9e7Z78MEH3bFjx8Z41bZ52dOjR4+6++67z82ZM8elpaW5QCDg7rjjDvef//xn7Bdu0Jtvvjnk/xe/2cOKigp35ZVXHndMfn6+S01NdbNnz3Z//vOfPZ+Xj24EAMCIcf+dMgAA+BpRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI/4fqv8jnFVNxEQAAAAASUVORK5CYII=\n","text/plain":["<Figure size 1200x400 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Colab Cell 8: Plotting Training History\n","\n","import matplotlib.pyplot as plt\n","\n","# The 'history' object contains the training metrics from Cell 6\n","# Note: Since this was random data, the plots will look messy, but the code is correct.\n","\n","# 1. Plot Training and Validation Loss\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Loss Curves')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# 2. Plot Training and Validation Accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Accuracy Curves')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.show()\n","print(\"Training history plots generated.\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiKW4STWRP7CCSq/NZuPPD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}